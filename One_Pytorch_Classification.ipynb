{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- pip install torchvision-------------------------\n",
    "\n",
    "# in this example we going to develop a classifier using torch python library and \n",
    "# writing the complete rutine (architecture, pre-processing, processing, validation and prediction) from the scratch\n",
    "#lets start then importung principal librearies:\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "# now lets import our dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# creamos un directorio para guardar el dataset\n",
    "data_mnist=datasets.MNIST(\n",
    "    root= 'datos', # where root is the root directry on which where we are\n",
    "    train= True, \n",
    "    download= True, \n",
    "    transform= ToTensor()\n",
    ")\n",
    "#now we have our 60k data items available and transformed to tensors using ToTensor\n",
    "#have GPU?\n",
    "device=(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(f'usando {device}')\n",
    "# in this case \"device\" is the CUDA variable for GPU when is available, but \n",
    "# for now we'll working using CPU only\n",
    "\n",
    "rows, cols = 3, 3\n",
    "#Lets see our data content:\n",
    "figure=plt.figure(figsize=(8,8))\n",
    "for i in range(1, cols*rows +1):\n",
    "    sample_id=torch.randint(len(data_mnist), size=(1,)).item()\n",
    "    img, lebel= data_mnist[sample_id]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(str(lebel))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.squeeze(), cmap='CMRmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x258ba73ea30>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training, validation and test datasets (80%, 10%, 10%): we need to create 3 partitions over the actual dataset to achive our goals. To do so...\n",
    "torch.manual_seed(123) #lets take the randomness control\n",
    "#now we can recreate the model independently of the running times.\n",
    "\n",
    "train, val, test=torch.utils.data.random_split(\n",
    "    data_mnist, [0.8 ,0.1, 0.1]\n",
    ")\n",
    "print(f'training set: {len(train)}')\n",
    "print(f'training val: {len(val)}')\n",
    "print(f'training test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as any NN doesn't accept any type of images. a nn accept vectores only, \n",
    "# then the first vector it's ging to have 28X28 elements i.e. 784 elements (flaten)\n",
    "#hide layer Nlayer + RELU (15 nodes) last layer 10 nodes softmax\n",
    "#In pytorch includint transformers, we use nn module\n",
    "\n",
    "#any neural network architecture is actually a sublass/class of nn.Module\n",
    "#alwais have to add 2 METHODES to this new CLASS: 1.- init (it defines the actual architecture of the neural network)\n",
    "# 2.- \"forward methode\" (tells us how each predition will be generated)\n",
    "# import torch.nn as nn\n",
    "#create Methode INIT by using def in Class definition\n",
    "\n",
    "class neuralnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #we use self along with nn. to add sequentially avery layer we have\n",
    "        #as we said we don't have vectors but images so we need to flat it using flatten\n",
    "        #by self \"object\" (in neural net) we asosiate a new variable called \"flatit\"\n",
    "        self.flatit=nn.Flatten()\n",
    "        self.net=nn.Sequential(#allows to interconect all the layers, net is the atribute that contains all the tree net layers\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.Linear(25, 5),\n",
    "            nn.Softmax()        ##Softmax normalize into (0, 1) to interpret as probability\n",
    "        )\n",
    "    def forward(self, x):#how our neural network actually process the data\n",
    "        #x: entry data item\n",
    "        x=self.flatit(x)\n",
    "        logits=self.net(x) #  numeric values (10) (the output of our network)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuralnet(\n",
      "  (flatit): Flatten(start_dim=1, end_dim=-1)\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=25, bias=True)\n",
      "    (3): Linear(in_features=25, out_features=5, bias=True)\n",
      "    (4): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "the number of parameters in the neural network arch is 40655\n"
     ]
    }
   ],
   "source": [
    "model=neuralnet()# we are generating a new instance of the neural network by () writed in model var\n",
    "print(model)\n",
    "t=sum(p.numel() for p in  model.parameters())\n",
    "print(f'the number of parameters in the neural network arch is {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'int'>\n",
      "we procced to transform int type into tensor to be properly processed by te network...\n",
      "now our lbl is a <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "#forward propagation:\n",
    "# is the frst predition we gonna have and its not going to be really good itis like this because we need additional tools like back propagation and the gradiant descending/ce\n",
    "img, lbl=train[201]\n",
    "print(type(img))\n",
    "print(type(lbl))\n",
    "print('we procced to transform int type into tensor to be properly processed by te network...')\n",
    "lbl=torch.tensor(lbl).reshape(1)\n",
    "print(f'now our lbl is a {type(lbl)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = img.to(device), lbl.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ARMANDO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1842, 0.1750, 0.1833, 0.2138, 0.2437]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=model(img)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:tensor([[0.1842, 0.1750, 0.1833, 0.2138, 0.2437]], grad_fn=<SoftmaxBackward0>)\n",
      "predicted category: 4\n",
      "actual category: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbzElEQVR4nO3df3DU9b3v8dfyI8uvZDGEZLMlYECFViCtCGlGpSi5hHQOF4TxgNoZsF4cMNhC6o9Jq6Btz8TiOeroUJg515J6joByR+DIWO5gMOFoE7ygDIdWcwhNJRgSKuewG4IESj73D8YtK0H4Lru8k/B8zHxnyO73ne/Hb3fy7JddvvE555wAALjKelkvAABwbSJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARB/rBXxVR0eHmpqalJqaKp/PZ70cAIBHzjm1trYqFAqpV6+LX+d0uQA1NTUpJyfHehkAgCvU2NioYcOGXfT5Lheg1NRUSecWnpaWZrwaAIBXkUhEOTk50Z/nF5O0AK1atUrPPfecmpublZeXp5dfflmTJk265NyXf+2WlpZGgACgG7vU2yhJ+RDC66+/rtLSUq1YsUIffvih8vLyVFRUpKNHjybjcACAbigpAXr++ee1cOFCPfDAA/rWt76lNWvWaMCAAfrNb36TjMMBALqhhAfo9OnT2rNnjwoLC/92kF69VFhYqJqamgv2b29vVyQSidkAAD1fwgP0+eef6+zZs8rKyop5PCsrS83NzRfsX15erkAgEN34BBwAXBvM/yFqWVmZwuFwdGtsbLReEgDgKkj4p+AyMjLUu3dvtbS0xDze0tKiYDB4wf5+v19+vz/RywAAdHEJvwJKSUnRhAkTVFlZGX2so6NDlZWVKigoSPThAADdVFL+HVBpaanmz5+vW2+9VZMmTdKLL76otrY2PfDAA8k4HACgG0pKgObOnau//OUvWr58uZqbm/Xtb39b27Ztu+CDCQCAa5fPOeesF3G+SCSiQCCgcDjMnRAAoBu63J/j5p+CAwBcmwgQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBEH+sFANeipn/o7XkmWJXreebfWws8z0jSlF3/6nnmjYlLPc/c88ELnmfQc3AFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GakwHmaGss8zxQVXud55uODAzzPOPeZ5xnp/8QxI/Xq1c/zzLw9azzP/HWi95uy3vv//tHzDLomroAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABM+55yzXsT5IpGIAoGAwuGw0tLSrJeDa0zR6Lc9z7xTP8fzzIgRizzPbAr+yfPMmKwPPc9I0tMHl3ueWfmHH3me8fm83w/5s8dOeZ7JKj/jeQbxu9yf41wBAQBMECAAgImEB+jpp5+Wz+eL2caMGZPowwAAurmk/EK6m2++We+8887fDtKH33sHAIiVlDL06dNHwWAwGd8aANBDJOU9oAMHDigUCmnkyJG6//77dejQoYvu297erkgkErMBAHq+hAcoPz9fFRUV2rZtm1avXq2Ghgbdcccdam1t7XT/8vJyBQKB6JaTk5PoJQEAuqCEB6i4uFj33HOPxo8fr6KiIr399ts6fvy43njjjU73LysrUzgcjm6NjY2JXhIAoAtK+qcDBg8erJtuukn19fWdPu/3++X3+5O9DABAF5P0fwd04sQJHTx4UNnZ2ck+FACgG0l4gB599FFVV1frz3/+s37/+9/r7rvvVu/evXXvvfcm+lAAgG4s4X8Fd/jwYd177706duyYhg4dqttvv121tbUaOnRoog8FAOjGEh6gDRs2JPpbAlfNLf7PPM88OvE+zzM3//MBzzOhcVs9z8Trkc+e9Dzz9l3/4Hlmf/3PPM/0OZnieQZdE/eCAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJP0X0gHdyZM13/E8M3DgwiSsxFboG7/0PPPDgeWeZ0o9T0i3/tsPPc+8//AP4jiSFBr9r3HN4fJwBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3A0bOM/AgbdaL6HbumftTs8zpbd4P86hQ//b88yglf29H0iSXolvDJeHKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3IwWQGBkT4hiq8jwxZMgU74d54SfeZ5B0XAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSmAbqVPn0GeZ9LSCpOwElwproAAACYIEADAhOcA7dy5UzNmzFAoFJLP59PmzZtjnnfOafny5crOzlb//v1VWFioAwcOJGq9AIAewnOA2tralJeXp1WrVnX6/MqVK/XSSy9pzZo12rVrlwYOHKiioiKdOnXqihcLAOg5PH8Iobi4WMXFxZ0+55zTiy++qCeffFIzZ86UJL366qvKysrS5s2bNW/evCtbLQCgx0joe0ANDQ1qbm5WYeHfPnESCASUn5+vmpqaTmfa29sViURiNgBAz5fQADU3N0uSsrKyYh7PysqKPvdV5eXlCgQC0S0nJyeRSwIAdFHmn4IrKytTOByObo2NjdZLAgBcBQkNUDAYlCS1tLTEPN7S0hJ97qv8fr/S0tJiNgBAz5fQAOXm5ioYDKqysjL6WCQS0a5du1RQUJDIQwEAujnPn4I7ceKE6uvro183NDRo7969Sk9P1/Dhw7V06VL98pe/1I033qjc3Fw99dRTCoVCmjVrViLXDQDo5jwHaPfu3brzzjujX5eWlkqS5s+fr4qKCj3++ONqa2vTQw89pOPHj+v222/Xtm3b1K9fv8StGgDQ7fmcc856EeeLRCIKBAIKh8O8HwR0I01vDfU8kzPrhOeZrKy/8zzT1LTR8wzid7k/x80/BQcAuDYRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAhOdfxwAAndm4ojSOqZ97nvj4hj/FcRx0RVwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkpgAs0Nz/reeb5Y1fnJqF//c4nV+U4SD6ugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFDhP038u8D7Uu7/nkdCo1d6PcxWl/P2bnmcOH/6PJKwEPRlXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5Girg1lff2PPOP636ThJUkzr80f+Z5JiUlw/PM68Pv9zwzcv1QzzPpwSWeZyTpR395Ko6pv/c8kZk53fNM+xN5nmfQNXEFBAAwQYAAACY8B2jnzp2aMWOGQqGQfD6fNm/eHPP8ggUL5PP5Yrbp071fZgMAejbPAWpra1NeXp5WrVp10X2mT5+uI0eORLf169df0SIBAD2P5w8hFBcXq7i4+Gv38fv9CgaDcS8KANDzJeU9oKqqKmVmZmr06NFavHixjh07dtF929vbFYlEYjYAQM+X8ABNnz5dr776qiorK/WrX/1K1dXVKi4u1tmzZzvdv7y8XIFAILrl5OQkekkAgC4o4f8OaN68edE/jxs3TuPHj9eoUaNUVVWlqVOnXrB/WVmZSktLo19HIhEiBADXgKR/DHvkyJHKyMhQfX19p8/7/X6lpaXFbACAni/pATp8+LCOHTum7OzsZB8KANCNeP4ruBMnTsRczTQ0NGjv3r1KT09Xenq6nnnmGc2ZM0fBYFAHDx7U448/rhtuuEFFRUUJXTgAoHvzHKDdu3frzjvvjH795fs38+fP1+rVq7Vv3z799re/1fHjxxUKhTRt2jT94he/kN/vT9yqAQDdns8556wXcb5IJKJAIKBwOMz7QXEIh7d5nll/1664jvXYf27wPHPy5J/jOhakjIxCzzOf3v5hXMcauPm/4prz6pOCyZ5nbnzv/yZhJUiky/05zr3gAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIK7YfcwHVPGeJ7p+++fxnWstLRve55ZlPNDzzOP3L/Y88zVlLVjhOeZwb9P8TzTE+8kfv9o73dU/1Xlf3ieyc5+0vMM4sfdsAEAXRoBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKbkfYw111X4HkmEtkb17Ea1/XzPBOa+99xHaunmT1mu+eZLQf+ZxJWYqtPn0GeZ75zfbnnmdq6/+V5BvHjZqQAgC6NAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDRx3oB6MYKFlqvIOHOnGn2PNPWVut55pQb4HkmHtdfvySuuQ/+7tcJXknn2jP/y/NM6GfcWLSn4AoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUh7mH79gp5nIpH4jjXzf4z0PPNW9bOeZwYNmuJ5JvLxMs8zkvTig95vdPlPf4jvhp9ejRr1qOeZqgfL4zrWkCf+Gtcc4AVXQAAAEwQIAGDCU4DKy8s1ceJEpaamKjMzU7NmzVJdXV3MPqdOnVJJSYmGDBmiQYMGac6cOWppaUnoogEA3Z+nAFVXV6ukpES1tbXavn27zpw5o2nTpqmtrS26z7Jly/TWW29p48aNqq6uVlNTk2bPnp3whQMAujdPH0LYtm1bzNcVFRXKzMzUnj17NHnyZIXDYb3yyitat26d7rrrLknS2rVr9c1vflO1tbX67ne/m7iVAwC6tSt6DygcDkuS0tPTJUl79uzRmTNnVFhYGN1nzJgxGj58uGpqajr9Hu3t7YpEIjEbAKDniztAHR0dWrp0qW677TaNHTtWktTc3KyUlBQNHjw4Zt+srCw1Nzd3+n3Ky8sVCASiW05OTrxLAgB0I3EHqKSkRPv379eGDRuuaAFlZWUKh8PRrbGx8Yq+HwCge4jrH6IuWbJEW7du1c6dOzVs2LDo48FgUKdPn9bx48djroJaWloUDHb+DyT9fr/8fn88ywAAdGOeroCcc1qyZIk2bdqkHTt2KDc3N+b5CRMmqG/fvqqsrIw+VldXp0OHDqmgoCAxKwYA9AieroBKSkq0bt06bdmyRampqdH3dQKBgPr3769AIKAHH3xQpaWlSk9PV1pamh555BEVFBTwCTgAQAxPAVq9erUkacqUKTGPr127VgsWLJAkvfDCC+rVq5fmzJmj9vZ2FRUV6de//nVCFgsA6Dl8zjlnvYjzRSIRBQIBhcNhpaWlWS+n2wkvvM7zTNa/9IvrWGfOHI9rrqdJScnwPPNvedM8z9w8o8LzTOhnZz3PAFfqcn+Ocy84AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmIjrN6Ki6wr88397nvnTw1PjOtbL8+d5nln5hx/FdSyv3p80M665Wwfv8jxzrOLHnmeyspZ6npFeiWMG6Lq4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856EeeLRCIKBAIKh8NKS0uzXg4AwKPL/TnOFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwlOAysvLNXHiRKWmpiozM1OzZs1SXV1dzD5TpkyRz+eL2RYtWpTQRQMAuj9PAaqurlZJSYlqa2u1fft2nTlzRtOmTVNbW1vMfgsXLtSRI0ei28qVKxO6aABA99fHy87btm2L+bqiokKZmZnas2ePJk+eHH18wIABCgaDiVkhAKBHuqL3gMLhsCQpPT095vHXXntNGRkZGjt2rMrKynTy5MmLfo/29nZFIpGYDQDQ83m6AjpfR0eHli5dqttuu01jx46NPn7fffdpxIgRCoVC2rdvn5544gnV1dXpzTff7PT7lJeX65lnnol3GQCAbsrnnHPxDC5evFi/+93v9N5772nYsGEX3W/Hjh2aOnWq6uvrNWrUqAueb29vV3t7e/TrSCSinJwchcNhpaWlxbM0AIChSCSiQCBwyZ/jcV0BLVmyRFu3btXOnTu/Nj6SlJ+fL0kXDZDf75ff749nGQCAbsxTgJxzeuSRR7Rp0yZVVVUpNzf3kjN79+6VJGVnZ8e1QABAz+QpQCUlJVq3bp22bNmi1NRUNTc3S5ICgYD69++vgwcPat26dfr+97+vIUOGaN++fVq2bJkmT56s8ePHJ+U/AADQPXl6D8jn83X6+Nq1a7VgwQI1NjbqBz/4gfbv36+2tjbl5OTo7rvv1pNPPnnZ7+dc7t8dAgC6pqS8B3SpVuXk5Ki6utrLtwQAXKO4FxwAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwEQf6wV8lXNOkhSJRIxXAgCIx5c/v7/8eX4xXS5Ara2tkqScnBzjlQAArkRra6sCgcBFn/e5SyXqKuvo6FBTU5NSU1Pl8/linotEIsrJyVFjY6PS0tKMVmiP83AO5+EczsM5nIdzusJ5cM6ptbVVoVBIvXpd/J2eLncF1KtXLw0bNuxr90lLS7umX2Bf4jycw3k4h/NwDufhHOvz8HVXPl/iQwgAABMECABgolsFyO/3a8WKFfL7/dZLMcV5OIfzcA7n4RzOwznd6Tx0uQ8hAACuDd3qCggA0HMQIACACQIEADBBgAAAJrpNgFatWqXrr79e/fr1U35+vj744APrJV11Tz/9tHw+X8w2ZswY62Ul3c6dOzVjxgyFQiH5fD5t3rw55nnnnJYvX67s7Gz1799fhYWFOnDggM1ik+hS52HBggUXvD6mT59us9gkKS8v18SJE5WamqrMzEzNmjVLdXV1MfucOnVKJSUlGjJkiAYNGqQ5c+aopaXFaMXJcTnnYcqUKRe8HhYtWmS04s51iwC9/vrrKi0t1YoVK/Thhx8qLy9PRUVFOnr0qPXSrrqbb75ZR44ciW7vvfee9ZKSrq2tTXl5eVq1alWnz69cuVIvvfSS1qxZo127dmngwIEqKirSqVOnrvJKk+tS50GSpk+fHvP6WL9+/VVcYfJVV1erpKREtbW12r59u86cOaNp06apra0tus+yZcv01ltvaePGjaqurlZTU5Nmz55tuOrEu5zzIEkLFy6MeT2sXLnSaMUX4bqBSZMmuZKSkujXZ8+edaFQyJWXlxuu6upbsWKFy8vLs16GKUlu06ZN0a87OjpcMBh0zz33XPSx48ePO7/f79avX2+wwqvjq+fBOefmz5/vZs6cabIeK0ePHnWSXHV1tXPu3P/2ffv2dRs3bozu8/HHHztJrqamxmqZSffV8+Ccc9/73vfcj3/8Y7tFXYYufwV0+vRp7dmzR4WFhdHHevXqpcLCQtXU1BiuzMaBAwcUCoU0cuRI3X///Tp06JD1kkw1NDSoubk55vURCASUn59/Tb4+qqqqlJmZqdGjR2vx4sU6duyY9ZKSKhwOS5LS09MlSXv27NGZM2diXg9jxozR8OHDe/Tr4avn4UuvvfaaMjIyNHbsWJWVlenkyZMWy7uoLncz0q/6/PPPdfbsWWVlZcU8npWVpU8++cRoVTby8/NVUVGh0aNH68iRI3rmmWd0xx13aP/+/UpNTbVenonm5mZJ6vT18eVz14rp06dr9uzZys3N1cGDB/XTn/5UxcXFqqmpUe/eva2Xl3AdHR1aunSpbrvtNo0dO1bSuddDSkqKBg8eHLNvT349dHYeJOm+++7TiBEjFAqFtG/fPj3xxBOqq6vTm2++abjaWF0+QPib4uLi6J/Hjx+v/Px8jRgxQm+88YYefPBBw5WhK5g3b170z+PGjdP48eM1atQoVVVVaerUqYYrS46SkhLt37//mngf9Otc7Dw89NBD0T+PGzdO2dnZmjp1qg4ePKhRo0Zd7WV2qsv/FVxGRoZ69+59wadYWlpaFAwGjVbVNQwePFg33XST6uvrrZdi5svXAK+PC40cOVIZGRk98vWxZMkSbd26Ve+++27Mr28JBoM6ffq0jh8/HrN/T309XOw8dCY/P1+SutTrocsHKCUlRRMmTFBlZWX0sY6ODlVWVqqgoMBwZfZOnDihgwcPKjs723opZnJzcxUMBmNeH5FIRLt27brmXx+HDx/WsWPHetTrwzmnJUuWaNOmTdqxY4dyc3Njnp8wYYL69u0b83qoq6vToUOHetTr4VLnoTN79+6VpK71erD+FMTl2LBhg/P7/a6iosL98Y9/dA899JAbPHiwa25utl7aVfWTn/zEVVVVuYaGBvf++++7wsJCl5GR4Y4ePWq9tKRqbW11H330kfvoo4+cJPf888+7jz76yH366afOOeeeffZZN3jwYLdlyxa3b98+N3PmTJebm+u++OIL45Un1tedh9bWVvfoo4+6mpoa19DQ4N555x13yy23uBtvvNGdOnXKeukJs3jxYhcIBFxVVZU7cuRIdDt58mR0n0WLFrnhw4e7HTt2uN27d7uCggJXUFBguOrEu9R5qK+vdz//+c/d7t27XUNDg9uyZYsbOXKkmzx5svHKY3WLADnn3Msvv+yGDx/uUlJS3KRJk1xtba31kq66uXPnuuzsbJeSkuK+8Y1vuLlz57r6+nrrZSXdu+++6yRdsM2fP985d+6j2E899ZTLyspyfr/fTZ061dXV1dkuOgm+7jycPHnSTZs2zQ0dOtT17dvXjRgxwi1cuLDH/Z+0zv77Jbm1a9dG9/niiy/cww8/7K677jo3YMAAd/fdd7sjR47YLToJLnUeDh065CZPnuzS09Od3+93N9xwg3vsscdcOBy2XfhX8OsYAAAmuvx7QACAnokAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMPH/AcY48kX/7TNmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict=logits.argmax(1)\n",
    "# ypredict\n",
    "plt.imshow(img.cpu().squeeze(), cmap='CMRmap_r');\n",
    "print(f'logits:{logits}')\n",
    "print(f'predicted category: {y_predict[0]}')\n",
    "print(f'actual category: {lbl[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6265, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Back propagation:\n",
    "fn_loss=nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.3) #gradient descending, learning rate of 30%\n",
    "loss=fn_loss(logits, lbl)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets calculate de gradients of the loss i.e (calculate the change of the loss regarding to model parameters)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batchsize=2000\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    dataset=train, \n",
    "    batch_size=batchsize, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader=DataLoader(\n",
    "    dataset=val,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training yn validation of the model\n",
    "#lets take our forward and back propagation and set it in a loop for to iterate it\n",
    "#hiperparameters...\n",
    "learning_rate=0.1\n",
    "epochs=10 #\"the epoch the number of time we want to execute forward-backward\"\n",
    "#lets build our loss function\n",
    "fn_loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[226], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m             ndata \u001b[38;5;241m=\u001b[39m nbatch\u001b[38;5;241m*\u001b[39mbatchsize\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mloss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndata\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss_train\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m nbatches\n\u001b[0;32m     25\u001b[0m accuracy \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m train_size\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAccuracy/loss (avg)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_train' is not defined"
     ]
    }
   ],
   "source": [
    "#the best practice is to generate two principal funcions : one to train and one more for the validatoin then set them into the 10 for loop \n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    train_size=len(dataloader.dataset)\n",
    "    nbatches=len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_accum, accuracy_accum= 0, 0\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y=X.to(device), y.to(device)\n",
    "        logits=model(X)\n",
    "        loss=fn_loss(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "        accuracy += (logits.argmax(1)==y).type(torch.float).sum().item()\n",
    "        if nbatch %10==0:\n",
    "            ndata = nbatch*batchsize\n",
    "            print(f'\\tloss:{loss.item():>7f} [{ndata:>5d}/{train_size:>5d}]')\n",
    "\n",
    "loss_train /= nbatches\n",
    "accuracy /= train_size\n",
    "print(f'\\Accuracy/loss (avg)')\n",
    "print(f'\\t\\tAccuracy/losss (average) {(100*accuracy):>0.1f}%/{loss_train:>8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[227], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m             loss_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mfn_loss(logits, y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     14\u001b[0m             exactitud \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m==\u001b[39my)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss_val\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m nbatches\n\u001b[0;32m     17\u001b[0m accuracy \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m val_size\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# print(f'\\Accuracy/loss (avg)')\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_val' is not defined"
     ]
    }
   ],
   "source": [
    "def val_loop(dataloader, model, fn_loss):\n",
    "    val_size=len(dataloader.dataset)\n",
    "    nbatches=len(dataloader)\n",
    "    model.eval()\n",
    "    loss_val, accuracy= 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for X,y in dataloader:\n",
    "            X, y=X.to(device)\n",
    "            logits=model(X)\n",
    "\n",
    "            loss_val +=fn_loss(logits, y).item()\n",
    "            exactitud +=(logits.argmax(1)==y).type(torch.float).sum().item()\n",
    "\n",
    "loss_val /= nbatches\n",
    "accuracy /= val_size\n",
    "\n",
    "# print(f'\\Accuracy/loss (avg)')\n",
    "print(f'\\t\\tAccuracy/losss (average) {(100*accuracy):>0.1f}%/{loss_val:>8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteracion1/10\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ARMANDO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 8 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteracion\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     val_loop(val_loader, model, fn_loss)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meso!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[226], line 13\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m X, y\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m logits\u001b[38;5;241m=\u001b[39mmodel(X)\n\u001b[1;32m---> 13\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[43mfn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ARMANDO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ARMANDO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ARMANDO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ARMANDO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 8 is out of bounds."
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f'iteracion{t+1}/{epochs}\\n----------------')\n",
    "    train_loop(train_loader, model, fn_loss, optimizer)\n",
    "    val_loop(val_loader, model, fn_loss)\n",
    "print('eso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
